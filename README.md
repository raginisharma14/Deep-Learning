# Deep-Learning
Deep Learning Mini Projects

Batch Gradient Descent - Entire Dataset to be read into memory. Each iterations wwalks through the entire dataset.
Stochastic Gradient descent - Each iteration walks through only one sample. Leraning rate is same for all the itertaions, 
Hence it doesn't converge to a global minimum. So try decreasing learning rate as no of iterations increase. 
